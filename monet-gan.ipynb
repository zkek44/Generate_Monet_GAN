{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":31041,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Monet GAN Kaggle Competition","metadata":{}},{"cell_type":"markdown","source":"https://github.com/zkek44/Generate_Monet_GAN","metadata":{}},{"cell_type":"markdown","source":"## Problem and Data Description\n\nThe goal of this Kaggle competition is to generate Monet-style paintings using GANs (Generative Adversarial Networks). Specifically, I need to build a GAN-based model capable of translating real-world photographs into paintings that resemble the style of Claude Monet.\n\nThe dataset includes two domains of 256x256 RGB images:\n\nmonet_jpg (300 images): Paintings by Claude Monet — used as the target domain for the style transfer.\n\nphoto_jpg (7,028 images): Real-world photographs — used as the source domain to be converted into Monet-style images.\n\nThe same sets are also provided as TFRecords (monet_tfrec and photo_tfrec).","metadata":{}},{"cell_type":"code","source":"import shutil\nimport os\n\nworking_dir = \"/kaggle/working\"\n\nfor item in os.listdir(working_dir):\n    path = os.path.join(working_dir, item)\n    try:\n        if os.path.isfile(path) or os.path.islink(path):\n            os.unlink(path)\n        elif os.path.isdir(path):\n            shutil.rmtree(path)\n    except Exception as e:\n        print(f\"Failed to delete {path}: {e}\")\n\nprint(\"✅ /kaggle/working cleaned.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-15T16:03:30.618571Z","iopub.execute_input":"2025-05-15T16:03:30.618789Z","iopub.status.idle":"2025-05-15T16:03:30.627982Z","shell.execute_reply.started":"2025-05-15T16:03:30.618771Z","shell.execute_reply":"2025-05-15T16:03:30.627083Z"}},"outputs":[{"name":"stdout","text":"✅ /kaggle/working cleaned.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"## Import Libraries","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport os\nimport numpy as np\nfrom tqdm.notebook import tqdm\nimport glob\nimport shutil","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T02:10:47.437586Z","iopub.execute_input":"2025-05-14T02:10:47.438271Z","iopub.status.idle":"2025-05-14T02:10:47.442580Z","shell.execute_reply.started":"2025-05-14T02:10:47.438236Z","shell.execute_reply":"2025-05-14T02:10:47.441820Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load Data","metadata":{}},{"cell_type":"code","source":"monet_dir = '/kaggle/input/gan-getting-started/monet_jpg'\nphoto_dir = '/kaggle/input/gan-getting-started/photo_jpg'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T02:10:47.444102Z","iopub.execute_input":"2025-05-14T02:10:47.444347Z","iopub.status.idle":"2025-05-14T02:10:47.455346Z","shell.execute_reply.started":"2025-05-14T02:10:47.444331Z","shell.execute_reply":"2025-05-14T02:10:47.454613Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"IMG_HEIGHT = 256\nIMG_WIDTH = 256\nBATCH_SIZE = 1\nAUTOTUNE = tf.data.AUTOTUNE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T02:10:47.456122Z","iopub.execute_input":"2025-05-14T02:10:47.456969Z","iopub.status.idle":"2025-05-14T02:10:47.468542Z","shell.execute_reply.started":"2025-05-14T02:10:47.456945Z","shell.execute_reply":"2025-05-14T02:10:47.467804Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def normalize_img(img):\n    img = tf.cast(img, tf.float32)\n    return (img / 127.5) - 1  # Scale to [-1, 1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T02:10:47.470440Z","iopub.execute_input":"2025-05-14T02:10:47.470618Z","iopub.status.idle":"2025-05-14T02:10:47.481060Z","shell.execute_reply.started":"2025-05-14T02:10:47.470606Z","shell.execute_reply":"2025-05-14T02:10:47.480374Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def load_jpg(filename):\n    img = tf.io.read_file(filename)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, [IMG_HEIGHT, IMG_WIDTH])\n    return normalize_img(img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T02:10:47.481836Z","iopub.execute_input":"2025-05-14T02:10:47.482066Z","iopub.status.idle":"2025-05-14T02:10:47.493294Z","shell.execute_reply.started":"2025-05-14T02:10:47.482046Z","shell.execute_reply":"2025-05-14T02:10:47.492601Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"monet_files = tf.data.Dataset.list_files(monet_dir + '/*.jpg', seed=42)\nphoto_files = tf.data.Dataset.list_files(photo_dir + '/*.jpg', seed=42)\n\nmonet_ds = monet_files.map(load_jpg, num_parallel_calls=AUTOTUNE).cache().shuffle(300).batch(BATCH_SIZE).prefetch(AUTOTUNE)\nphoto_ds = photo_files.map(load_jpg, num_parallel_calls=AUTOTUNE).cache().shuffle(1000).batch(BATCH_SIZE).prefetch(AUTOTUNE)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T02:10:47.494073Z","iopub.execute_input":"2025-05-14T02:10:47.494630Z","iopub.status.idle":"2025-05-14T02:10:54.158341Z","shell.execute_reply.started":"2025-05-14T02:10:47.494608Z","shell.execute_reply":"2025-05-14T02:10:54.157601Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"markdown","source":"We can see that we have 300 Monet paintings that will be used to train the model and 7,038 photos to generate Monet-style paintings from.\n\nThere are no missing values and since all of the images are already resized to 256x256 so there isn't any more data cleaning that we need to perform.","metadata":{}},{"cell_type":"markdown","source":"## Model Building","metadata":{}},{"cell_type":"code","source":"def resnet_block(x, filters):\n    init = x\n    x = layers.Conv2D(filters, 3, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.ReLU()(x)\n    x = layers.Conv2D(filters, 3, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.add([x, init])\n    return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T02:10:54.159346Z","iopub.execute_input":"2025-05-14T02:10:54.159550Z","iopub.status.idle":"2025-05-14T02:10:54.163551Z","shell.execute_reply.started":"2025-05-14T02:10:54.159534Z","shell.execute_reply":"2025-05-14T02:10:54.162808Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_generator():\n    inputs = layers.Input(shape=[IMG_HEIGHT, IMG_WIDTH, 3])\n    x = layers.Conv2D(64, 7, padding='same')(inputs)\n    x = layers.ReLU()(x)\n    x = layers.Conv2D(128, 3, strides=2, padding='same')(x)\n    x = layers.ReLU()(x)\n    x = layers.Conv2D(256, 3, strides=2, padding='same')(x)\n    x = layers.ReLU()(x)\n    for _ in range(9): x = resnet_block(x, 256)\n    x = layers.Conv2DTranspose(128, 3, strides=2, padding='same')(x)\n    x = layers.ReLU()(x)\n    x = layers.Conv2DTranspose(64, 3, strides=2, padding='same')(x)\n    x = layers.ReLU()(x)\n    x = layers.Conv2D(3, 7, padding='same', activation='tanh')(x)\n    return tf.keras.Model(inputs, x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T02:10:54.164290Z","iopub.execute_input":"2025-05-14T02:10:54.164575Z","iopub.status.idle":"2025-05-14T02:10:54.174883Z","shell.execute_reply.started":"2025-05-14T02:10:54.164560Z","shell.execute_reply":"2025-05-14T02:10:54.174315Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def build_discriminator():\n    inp = layers.Input(shape=[IMG_HEIGHT, IMG_WIDTH, 3])\n    x = layers.Conv2D(64, 4, strides=2, padding='same')(inp)\n    x = layers.LeakyReLU(0.2)(x)\n    x = layers.Conv2D(128, 4, strides=2, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU(0.2)(x)\n    x = layers.Conv2D(256, 4, strides=2, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU(0.2)(x)\n    x = layers.Conv2D(512, 4, strides=1, padding='same')(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.LeakyReLU(0.2)(x)\n    x = layers.Conv2D(1, 4, strides=1, padding='same')(x)\n    return tf.keras.Model(inputs=inp, outputs=x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T02:10:54.175414Z","iopub.execute_input":"2025-05-14T02:10:54.175586Z","iopub.status.idle":"2025-05-14T02:10:54.186832Z","shell.execute_reply.started":"2025-05-14T02:10:54.175573Z","shell.execute_reply":"2025-05-14T02:10:54.186158Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Loss and Optimizer Functions","metadata":{}},{"cell_type":"code","source":"loss_obj = tf.keras.losses.MeanSquaredError()\nLAMBDA_CYCLE = 10\nLAMBDA_IDENTITY = 0.5 * LAMBDA_CYCLE\n\ndef generator_loss(fake_output):\n    return loss_obj(tf.ones_like(fake_output), fake_output)\n\ndef discriminator_loss(real_output, fake_output):\n    return 0.5 * (loss_obj(tf.ones_like(real_output), real_output) + \n                  loss_obj(tf.zeros_like(fake_output), fake_output))\n\ndef cycle_loss(real, cycled):\n    return tf.reduce_mean(tf.abs(real - cycled)) * LAMBDA_CYCLE\n\ndef identity_loss(real, same):\n    return tf.reduce_mean(tf.abs(real - same)) * LAMBDA_IDENTITY","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T02:10:54.189226Z","iopub.execute_input":"2025-05-14T02:10:54.189455Z","iopub.status.idle":"2025-05-14T02:10:54.200582Z","shell.execute_reply.started":"2025-05-14T02:10:54.189434Z","shell.execute_reply":"2025-05-14T02:10:54.199880Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generator_g = build_generator()\ngenerator_f = build_generator()\ndiscriminator_x = build_discriminator()\ndiscriminator_y = build_discriminator()\n\ngenerator_g_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ngenerator_f_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndiscriminator_x_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndiscriminator_y_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T02:10:54.201333Z","iopub.execute_input":"2025-05-14T02:10:54.201602Z","iopub.status.idle":"2025-05-14T02:10:55.637893Z","shell.execute_reply.started":"2025-05-14T02:10:54.201587Z","shell.execute_reply":"2025-05-14T02:10:55.637317Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Model Training","metadata":{}},{"cell_type":"code","source":"@tf.function\ndef train_step(real_x, real_y):\n    with tf.GradientTape(persistent=True) as tape:\n        fake_y = generator_g(real_x, training=True)\n        cycled_x = generator_f(fake_y, training=True)\n        fake_x = generator_f(real_y, training=True)\n        cycled_y = generator_g(fake_x, training=True)\n\n        same_x = generator_f(real_x, training=True)\n        same_y = generator_g(real_y, training=True)\n\n        disc_real_x = discriminator_x(real_x, training=True)\n        disc_real_y = discriminator_y(real_y, training=True)\n        disc_fake_x = discriminator_x(fake_x, training=True)\n        disc_fake_y = discriminator_y(fake_y, training=True)\n\n        gen_g_loss = generator_loss(disc_fake_y)\n        gen_f_loss = generator_loss(disc_fake_x)\n        total_cycle_loss = cycle_loss(real_x, cycled_x) + cycle_loss(real_y, cycled_y)\n        total_gen_g_loss = gen_g_loss + total_cycle_loss + identity_loss(real_y, same_y)\n        total_gen_f_loss = gen_f_loss + total_cycle_loss + identity_loss(real_x, same_x)\n        disc_x_loss = discriminator_loss(disc_real_x, disc_fake_x)\n        disc_y_loss = discriminator_loss(disc_real_y, disc_fake_y)\n\n    generator_g_optimizer.apply_gradients(zip(tape.gradient(total_gen_g_loss, generator_g.trainable_variables), generator_g.trainable_variables))\n    generator_f_optimizer.apply_gradients(zip(tape.gradient(total_gen_f_loss, generator_f.trainable_variables), generator_f.trainable_variables))\n    discriminator_x_optimizer.apply_gradients(zip(tape.gradient(disc_x_loss, discriminator_x.trainable_variables), discriminator_x.trainable_variables))\n    discriminator_y_optimizer.apply_gradients(zip(tape.gradient(disc_y_loss, discriminator_y.trainable_variables), discriminator_y.trainable_variables))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T02:10:55.638631Z","iopub.execute_input":"2025-05-14T02:10:55.638922Z","iopub.status.idle":"2025-05-14T02:10:55.645934Z","shell.execute_reply.started":"2025-05-14T02:10:55.638897Z","shell.execute_reply":"2025-05-14T02:10:55.645261Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import time\n\nEPOCHS = 50\n\nfor epoch in range(EPOCHS):\n    start = time.time()\n    for image_x, image_y in tf.data.Dataset.zip((monet_ds, photo_ds)):\n        train_step(image_x, image_y)\n    print(f'Epoch {epoch+1} completed in {time.time()-start:.2f} sec')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T02:10:55.646577Z","iopub.execute_input":"2025-05-14T02:10:55.646786Z","iopub.status.idle":"2025-05-14T05:23:06.787973Z","shell.execute_reply.started":"2025-05-14T02:10:55.646771Z","shell.execute_reply":"2025-05-14T05:23:06.787158Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Submission","metadata":{}},{"cell_type":"code","source":"OUTPUT_DIR = '/kaggle/working'\nos.makedirs(OUTPUT_DIR, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T05:23:06.789205Z","iopub.execute_input":"2025-05-14T05:23:06.789505Z","iopub.status.idle":"2025-05-14T05:23:06.793549Z","shell.execute_reply.started":"2025-05-14T05:23:06.789482Z","shell.execute_reply":"2025-05-14T05:23:06.792808Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def denormalize_img(img_tensor):\n    # Convert pixel range from [-1, 1] to [0, 255]\n    img = (img_tensor + 1) * 127.5\n    img = tf.clip_by_value(img, 0, 255)\n    return tf.cast(img, tf.uint8)\n\ndef generate_and_save_images(generator, photo_paths, output_dir):\n    i = 0\n    for path in tqdm(photo_paths):\n        image = load_jpg(path)\n        image = tf.expand_dims(image, 0)  # Add batch dimension\n\n        prediction = generator(image, training=False)\n        prediction = denormalize_img(prediction[0])\n\n        # Save as JPEG\n        output_path = os.path.join(output_dir, f\"{i}.jpg\")\n        Image.fromarray(prediction.numpy()).save(output_path)\n        i += 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T05:23:06.794345Z","iopub.execute_input":"2025-05-14T05:23:06.794616Z","iopub.status.idle":"2025-05-14T05:23:06.816236Z","shell.execute_reply.started":"2025-05-14T05:23:06.794601Z","shell.execute_reply":"2025-05-14T05:23:06.815632Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"photo_paths = sorted(glob.glob('/kaggle/input/gan-getting-started/photo_jpg/*.jpg'))[:7000]\ngenerate_and_save_images(generator_g, photo_paths, OUTPUT_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T05:23:06.816960Z","iopub.execute_input":"2025-05-14T05:23:06.817263Z","iopub.status.idle":"2025-05-14T05:31:13.374931Z","shell.execute_reply.started":"2025-05-14T05:23:06.817239Z","shell.execute_reply":"2025-05-14T05:31:13.374193Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"shutil.make_archive('images', 'zip', OUTPUT_DIR)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T05:31:13.375775Z","iopub.execute_input":"2025-05-14T05:31:13.375996Z","iopub.status.idle":"2025-05-14T05:31:16.134434Z","shell.execute_reply.started":"2025-05-14T05:31:13.375979Z","shell.execute_reply":"2025-05-14T05:31:16.133576Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Results","metadata":{}},{"cell_type":"code","source":"def show_sample(index):\n    img = load_jpg(photo_paths[index])\n    generated = generator_g(tf.expand_dims(img, 0), training=False)\n    generated = (generated[0] + 1) * 127.5\n\n    fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n    axs[0].imshow((img + 1) / 2)\n    axs[0].set_title(\"Original Photo\")\n    axs[1].imshow(generated.numpy().astype('uint8'))\n    axs[1].set_title(\"Monet-style Output\")\n    for ax in axs:\n        ax.axis('off')\n    plt.show()\n\nshow_sample(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-14T05:31:16.135339Z","iopub.execute_input":"2025-05-14T05:31:16.136091Z","iopub.status.idle":"2025-05-14T05:31:16.468439Z","shell.execute_reply.started":"2025-05-14T05:31:16.136068Z","shell.execute_reply":"2025-05-14T05:31:16.467715Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"I trained a CycleGAN model for 50 epochs using Monet paintings and real-world photos. The generator learned to map photo images to the Monet style using adversarial, cycle-consistency, and identity losses. The training was slow on CPU but accelerated substantially when moved to Google Colab with GPU support.\n\nThe model successfully captured the Monet style with soft brush strokes, pastel tones, and texture patterns. Generated images showed meaningful transformations while preserving content (e.g., shapes, trees, sky). Identity and cycle-consistency losses helped maintain structure and color integrity.\n\nHyperparameter Settings: Optimizers: Adam (lr=2e-4, β₁=0.5) Loss Weights: Cycle = 10, Identity = 5 Batch Size: 1 (common in GAN training) Epochs: 50\n\nWhat Helped: Using ResNet blocks in the generator improved detail preservation. Batch normalization stabilized training. Training with image normalization to [-1, 1] was essential for tanh activation.\n\nChallenges: Training on CPU was unfeasibly slow. GPU memory constraints forced us to use batch size of 1. Without TFRecord + TPU acceleration, full convergence is slow.\n\nFuture Work: Use TFRecords with TPU for speed and scalability Add perceptual loss for better visual fidelity Use style attention layers for more nuanced style transfer","metadata":{}}]}